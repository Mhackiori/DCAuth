{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîê Authentication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shap\n",
    "import sys\n",
    "import tsfresh\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils.const import *\n",
    "from utils.helperFunctions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Also affect subprocesses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìç Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing IDs\n",
    "ids = ['id1', 'id2', 'id3']\n",
    "# Choose what ID to process\n",
    "id = 'id1'\n",
    "ids_remove = [x for x in ids if x != id]\n",
    "\n",
    "# Filter features and keep only relevant ones\n",
    "filterFeatures = True\n",
    "\n",
    "# Undersampling\n",
    "fairUndersampling = False       # Each class same number\n",
    "targetedUndersampling = True    # Downsample most frequent class\n",
    "customBalance = False           # Downsample by specifying number of samples for each label\n",
    "\n",
    "# Choose whether to separate Charge and Discharge cycling or not\n",
    "separate = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'AdaBoost',\n",
    "    'Decision Tree',\n",
    "    'Gaussian Naive Bayes',\n",
    "    'Nearest Neighbors',\n",
    "    'Neural Network',\n",
    "    'Quadratic Discriminant Analysis',\n",
    "    'Random Forest',\n",
    "    'Support Vector Machine'\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    AdaBoostClassifier(random_state=SEED),\n",
    "    DecisionTreeClassifier(random_state=SEED),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    MLPClassifier(random_state=SEED),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    RandomForestClassifier(random_state=SEED),\n",
    "    SVC(random_state=SEED),\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "    # AdaBoostClassifier\n",
    "    {\n",
    "        'n_estimators': [50, 100, 150, 200]\n",
    "    },\n",
    "    # DecisionTreeClassifier\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': np.arange(3, 20)\n",
    "    },\n",
    "    # GaussianNB\n",
    "    {\n",
    "        'var_smoothing': np.logspace(0, -9, num=100)\n",
    "    },\n",
    "    # KNeighborsClassifier\n",
    "    {\n",
    "        'n_neighbors': list(range(1, 20)),\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    # MLPClassifier\n",
    "    {\n",
    "        'hidden_layer_sizes': [(50, ), (100, ), (200, )],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd']\n",
    "    },\n",
    "    # QuadraticDiscriminantAnalysis\n",
    "    {\n",
    "        'reg_param': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    },\n",
    "    # RandomForestClassifier\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'n_estimators': [100, 200, 300, 400, 500]\n",
    "    },\n",
    "    # SVC\n",
    "    {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': np.arange(1, 5, 1),\n",
    "        'gamma': np.arange(0.2, 1, 0.2)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not separate:\n",
    "    dff = []\n",
    "    for dataset in DATASETS:\n",
    "        dir = os.path.join(PROCESSED, dataset)\n",
    "        for file in os.listdir(dir):\n",
    "            if file.split('.')[-1] == 'parquet':\n",
    "                df = pd.read_parquet(os.path.join(dir, file))\n",
    "                dff.append(df)\n",
    "\n",
    "        df = pd.concat(dff)\n",
    "else:\n",
    "    dff_charge = []\n",
    "    dff_discharge = []\n",
    "    for dataset in DATASETS:\n",
    "        dir = os.path.join(PROCESSED, dataset)\n",
    "        for file in os.listdir(dir):\n",
    "            if file.split('.')[-1] == 'parquet':\n",
    "                df = pd.read_parquet(os.path.join(dir, file))\n",
    "                if file.split('.')[0] == 'charge':\n",
    "                    dff_charge.append(df)\n",
    "                elif file.split('.')[0] == 'discharge':\n",
    "                    dff_discharge.append(df)\n",
    "\n",
    "    df_charge = pd.concat(dff_charge)\n",
    "    df_discharge = pd.concat(dff_discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not separate:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    df['id1'].value_counts().sort_index().plot(\n",
    "        kind='bar', title='ID1 Distribution', xlabel='IDs', ylabel='Occurences', ax=axs[0])\n",
    "    df['id2'].value_counts().sort_index().plot(\n",
    "        kind='bar', title='ID2 Distribution', xlabel='IDs', ylabel='Occurences', ax=axs[1])\n",
    "\n",
    "else:\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    df_charge['id1'].value_counts().sort_index().plot(\n",
    "        kind='bar', title='ID1 Distribution on Charge', xlabel='IDs', ylabel='Occurences', ax=axs[0])\n",
    "    df_charge['id2'].value_counts().sort_index().plot(\n",
    "        kind='bar', title='ID2 Distribution on Charge', xlabel='IDs', ylabel='Occurences', ax=axs[1])\n",
    "    df_discharge['id1'].value_counts().sort_index().plot(\n",
    "        kind='bar', title='ID1 Distribution on Discharge', xlabel='IDs', ylabel='Occurences', ax=axs[2])\n",
    "    df_discharge['id2'].value_counts().sort_index().plot(\n",
    "        kind='bar', title='ID2 Distribution on Discharge', xlabel='IDs', ylabel='Occurences', ax=axs[3])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if targetedUndersampling:\n",
    "    if not separate:\n",
    "        df_x = df.drop(id, axis=1)\n",
    "        df_x = df_x.drop(ids_remove, axis=1)\n",
    "        \n",
    "        X_resampled, y_resampled = RandomUnderSampler(random_state=SEED).fit_resample(df_x, df[id])\n",
    "\n",
    "        X_resampled[id] = y_resampled\n",
    "        df = X_resampled\n",
    "\n",
    "        df[id].value_counts().sort_index().plot(\n",
    "            kind='bar', title='ID Distribution', xlabel='IDs', ylabel='Occurences')\n",
    "    else:\n",
    "        dfc_x = df_charge.drop(id, axis=1)\n",
    "        dfd_x = df_discharge.drop(id, axis=1)\n",
    "\n",
    "        dfc_x = dfc_x.drop(ids_remove, axis=1)\n",
    "        dfd_x = dfd_x.drop(ids_remove, axis=1)\n",
    "        \n",
    "        Xc_resampled, yc_resampled = RandomUnderSampler(random_state=SEED).fit_resample(dfc_x, df_charge[id])\n",
    "        Xd_resampled, yd_resampled = RandomUnderSampler(random_state=SEED).fit_resample(dfd_x, df_discharge[id])\n",
    "\n",
    "        Xc_resampled[id] = yc_resampled\n",
    "        Xd_resampled[id] = yd_resampled\n",
    "        \n",
    "        df_charge = Xc_resampled\n",
    "        df_discharge = Xd_resampled\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axs = axs.ravel()\n",
    "\n",
    "        df_charge[id].value_counts().sort_index().plot(\n",
    "            kind='bar', title='ID Distribution on Charge', xlabel='IDs', ylabel='Occurences', ax=axs[0])\n",
    "        df_discharge[id].value_counts().sort_index().plot(\n",
    "            kind='bar', title='ID Distribution on Discharge', xlabel='IDs', ylabel='Occurences', ax=axs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not separate:\n",
    "    beforeFeat = df.shape[1]\n",
    "    tsfresh.utilities.dataframe_functions.impute(df)\n",
    "\n",
    "    if filterFeatures:\n",
    "        df = tsfresh.select_features(df, df[id])\n",
    "        afterFeat = df.shape[1]\n",
    "\n",
    "        print(f'[üî• FILTER]\\n\\tBefore: {beforeFeat}\\n\\tAfter: {afterFeat}')\n",
    "else:\n",
    "    beforeFeat_c = df_charge.shape[1]\n",
    "    beforeFeat_d = df_discharge.shape[1]\n",
    "\n",
    "    tsfresh.utilities.dataframe_functions.impute(df_charge)\n",
    "    tsfresh.utilities.dataframe_functions.impute(df_discharge)\n",
    "\n",
    "    if filterFeatures:\n",
    "        df_charge = tsfresh.select_features(df_charge, df_charge[id])\n",
    "        df_discharge = tsfresh.select_features(df_discharge, df_discharge[id])\n",
    "        \n",
    "        afterFeat_c = df_charge.shape[1]\n",
    "        afterFeat_d = df_discharge.shape[1]\n",
    "\n",
    "        print(f'[üî• CHARGE]\\n\\tBefore: {beforeFeat_c}\\n\\tAfter: {afterFeat_c}')\n",
    "        print()\n",
    "        print(f'[üî• DISCHARGE]\\n\\tBefore: {beforeFeat_d}\\n\\tAfter: {afterFeat_d}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not separate:\n",
    "    # Loading labels\n",
    "    labels = df[id][:, np.newaxis]\n",
    "\n",
    "    # Loading features\n",
    "    features = df.drop(id, axis=1)\n",
    "    # for id_remove in ids_remove:\n",
    "    #     features = features.drop(id_remove, axis=1)\n",
    "\n",
    "    # Train and test split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    cols = []\n",
    "    for col in X_train.columns:\n",
    "        cols.append(col.replace('dQ/dV__', ''))\n",
    "\n",
    "    X_train.columns = cols\n",
    "    X_test.columns = cols\n",
    "else:\n",
    "    # Loading labels\n",
    "    labels_c = df_charge[id][:, np.newaxis]\n",
    "    labels_d = df_discharge[id][:, np.newaxis]\n",
    "\n",
    "    # Loading features\n",
    "    features_c = df_charge.drop(id, axis=1)\n",
    "    features_d = df_discharge.drop(id, axis=1)\n",
    "    # for id_remove in ids_remove:\n",
    "    #     features_c = features_c.drop(id_remove, axis=1)\n",
    "    #     features_d = features_d.drop(id_remove, axis=1)\n",
    "\n",
    "    # Train and test split\n",
    "    X_train_c, X_test_c, Y_train_c, Y_test_c = train_test_split(\n",
    "        features_c, labels_c, test_size=0.2, random_state=SEED)\n",
    "    X_train_d, X_test_d, Y_train_d, Y_test_d = train_test_split(\n",
    "        features_d, labels_d, test_size=0.2, random_state=SEED)\n",
    "    \n",
    "    cols_c = []\n",
    "    cols_d = []\n",
    "    for col in X_train_c.columns:\n",
    "        cols_c.append(col.replace('dQ/dV__', ''))\n",
    "    for col in X_train_d.columns:\n",
    "        cols_d.append(col.replace('dQ/dV__', ''))\n",
    "\n",
    "    X_train_c.columns = cols_c\n",
    "    X_test_c.columns = cols_c\n",
    "    X_train_d.columns = cols_d\n",
    "    X_test_d.columns = cols_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not separate:\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    best_params = []\n",
    "else:\n",
    "    train_scores_c = []\n",
    "    train_scores_d = []\n",
    "    test_scores_c = []\n",
    "    test_scores_d = []\n",
    "    best_params_c = []\n",
    "    best_params_d = []\n",
    "\n",
    "# Iterate over classifiers\n",
    "for name, clf, param in zip(names, classifiers, parameters):\n",
    "    print(f'[ü§ñ MODEL] {name}')\n",
    "\n",
    "    if not separate:\n",
    "        grid = GridSearchCV(clf, param, n_jobs=-1, verbose=0)\n",
    "        \n",
    "        grid.fit(X_train, Y_train)\n",
    "\n",
    "        score_train = grid.best_estimator_.score(X_train, Y_train)\n",
    "        print(f'\\t[üëü TRAIN]\\t{round(score_train, 3)}')\n",
    "\n",
    "        score_test = grid.best_estimator_.score(X_test, Y_test)\n",
    "        print(f'\\t[üß™ TEST]\\t{round(score_test, 3)}\\n')\n",
    "\n",
    "        train_scores.append(score_train)\n",
    "        test_scores.append(score_test)\n",
    "        best_params.append(grid.best_params_)\n",
    "\n",
    "        # Feature importance for Random Forest\n",
    "        if name == 'Random Forest':\n",
    "            impurity = grid.best_estimator_.feature_importances_\n",
    "            std = np.std([tree.feature_importances_ for tree in grid.best_estimator_.estimators_], axis=0)\n",
    "            explainer = shap.TreeExplainer(grid.best_estimator_)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "    else:\n",
    "        grid_c = GridSearchCV(clf, param, n_jobs=-1, verbose=0)\n",
    "        grid_d = GridSearchCV(clf, param, n_jobs=-1, verbose=0)\n",
    "\n",
    "        grid_c.fit(X_train_c, Y_train_c)\n",
    "        grid_d.fit(X_train_d, Y_train_d)\n",
    "\n",
    "        score_train_c = grid_c.best_estimator_.score(X_train_c, Y_train_c)\n",
    "        score_train_d = grid_d.best_estimator_.score(X_train_d, Y_train_d)\n",
    "        print(f'\\t[üëü TRAIN-C]\\t{round(score_train_c, 3)}')\n",
    "        print(f'\\t[üëü TRAIN-D]\\t{round(score_train_d, 3)}')\n",
    "\n",
    "        score_test_c = grid_c.best_estimator_.score(X_test_c, Y_test_c)\n",
    "        score_test_d = grid_d.best_estimator_.score(X_test_d, Y_test_d)\n",
    "        print(f'\\t[üß™ TEST-C]\\t{round(score_test_c, 3)}')\n",
    "        print(f'\\t[üß™ TEST-D]\\t{round(score_test_d, 3)}\\n')\n",
    "\n",
    "        train_scores_c.append(score_train_c)\n",
    "        train_scores_d.append(score_train_d)\n",
    "        test_scores_c.append(score_test_c)\n",
    "        test_scores_d.append(score_test_d)\n",
    "        best_params_c.append(grid_c.best_params_)\n",
    "        best_params_d.append(grid_d.best_params_)\n",
    "\n",
    "        # Feature importance for Random Forest\n",
    "        if name == 'Random Forest':\n",
    "            impurity_c = grid_c.best_estimator_.feature_importances_\n",
    "            impurity_d = grid_d.best_estimator_.feature_importances_\n",
    "            std_c = np.std([tree.feature_importances_ for tree in grid_c.best_estimator_.estimators_], axis=0)\n",
    "            std_d = np.std([tree.feature_importances_ for tree in grid_d.best_estimator_.estimators_], axis=0)\n",
    "            explainer_c = shap.TreeExplainer(grid_c.best_estimator_)\n",
    "            explainer_d = shap.TreeExplainer(grid_d.best_estimator_)\n",
    "            shap_values_c = explainer_c.shap_values(X_test_c)\n",
    "            shap_values_d = explainer_d.shap_values(X_test_d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîù Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not separate:\n",
    "    forest_impurity = pd.Series(impurity, index=X_train.columns).nlargest(20)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    forest_impurity.plot.bar(ax=ax)  # , yerr=std)\n",
    "    ax.set_title(\"Feature importances using MDI\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "else:\n",
    "    forest_impurity_c = pd.Series(impurity_c, index=X_train_c.columns).nlargest(20)\n",
    "    forest_impurity_d = pd.Series(impurity_d, index=X_train_d.columns).nlargest(20)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    forest_impurity_c.plot.bar(ax=axs[0])  # , yerr=std_c)\n",
    "    forest_impurity_d.plot.bar(ax=axs[1])  # , yerr=std_d)\n",
    "    axs[0].set_title(\"Feature importances using MDI on Charge\")\n",
    "    axs[0].set_ylabel(\"Mean decrease in impurity\")\n",
    "    axs[1].set_title(\"Feature importances using MDI on Discharge\")\n",
    "    axs[1].set_ylabel(\"Mean decrease in impurity\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not separate:\n",
    "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "else:\n",
    "    shap.summary_plot(shap_values_c, X_test_c, plot_type=\"bar\")\n",
    "    shap.summary_plot(shap_values_d, X_test_d, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
